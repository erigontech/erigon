name: QA - TxPool performance test

on:
  workflow_call:
  workflow_dispatch:
  push:
    branches:
      - "release/3.*"
  schedule:
    - cron: "0 0 * * 0"

jobs:
  tx_pool_assertoor_test:
    runs-on: [self-hosted, qa, X64, long-running]
    env:
      ERIGON_QA_PATH: /home/qarunner/erigon-qa
      ENCLAVE_NAME: "kurtosis-run-${{ github.run_id }}"

    steps:
      - name: Fast checkout git repository
        uses: actions/checkout@v4

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.ORG_DOCKERHUB_ERIGONTECH_USERNAME }}
          password: ${{ secrets.ORG_DOCKERHUB_ERIGONTECH_TOKEN }}

      - name: Docker build current branch
        run: |
          docker build -t test/erigon:current .

      - name: Run self hosted Kurtosis + assertoor tests
        id: erigon_kurtosis_test
        uses: erigontech/kurtosis-assertoor-github-action@v1.1.7
        with:
          enclave_name: ${{ env.ENCLAVE_NAME }}
          ethereum_package_args: ".github/workflows/kurtosis/txpool-assertoor.io"
          kurtosis_extra_args: --verbosity detailed --cli-log-level trace
          persistent_logs: "true"
          clean_docker: "true"

      - name: Parse Kurtosis output log and create JSON
        if: always()
        id: parse_kurtosis_output
        run: |
          # Find the folder starting with 'assertoor--'
          assertoor_folder=$(find ${{ runner.temp }}/${{ env.ENCLAVE_NAME }}/dump -type d -name 'assertoor--*' | head -n 1)

          # Exit if no assertoor folder is found
          if [ -z "$assertoor_folder" ]; then
            echo "No directory starting with 'assertoor--' found. Skipping."
            exit 0
          fi

          # Parse the output.log file within the assertoor-- folder
          output_log="${assertoor_folder}/output.log"
          echo "Parsing file: $output_log"

          # Create an empty JSON file if there are no results
          output_json="${{ github.workspace }}/outputs_kurtosis.json"
          echo "{}" > $output_json

          # Create a temporary directory
          tmp_dir=$(mktemp -d)

          # Parse the output.log file to find lines with msg="outputs_json"
          while IFS= read -r line; do
            if [[ "$line" == *'msg="outputs_json:'* ]]; then
              # Extract the JSON object and unescape the quotes
              json_str=$(echo "$line" | sed 's/.*msg="outputs_json: \(.*\)".*/\1/' | sed 's/\\"/"/g')
              
              # Extract the task value from the log line
              task=$(echo "$line" | sed -n 's/.*task=\([^ ]*\).*/\1/p')
              
              # Skip if task is empty
              if [ -n "$task" ]; then
                # Create or append to task file
                if [ -f "$tmp_dir/$task" ]; then
                  echo "," >> "$tmp_dir/$task"
                fi
                echo "$json_str" >> "$tmp_dir/$task"
              fi
            fi
          done < "$output_log"

          # Write the JSON object to the final file
          echo "{" > $output_json
          first_task=true
          for task_file in "$tmp_dir"/*; do
            [ -f "$task_file" ] || continue
            
            task=$(basename "$task_file")
            
            # Add comma for all but first task
            if [ "$first_task" = true ]; then
              first_task=false
            else
              echo "," >> $output_json
            fi
            
            # Write the task array to the file
            echo "\"$task\": [$(cat "$task_file")]" >> $output_json
          done
          echo "}" >> $output_json

          echo "outputs_kurtosis.json: $output_json"

          echo "success=true" >> $GITHUB_OUTPUT

          # Clean up
          rm -rf "$tmp_dir"
          echo "cleaned up"

      - name: Upload outputs_kurtosis.json as artifact
        if: always() && steps.parse_kurtosis_output.outputs.success == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: outputs_kurtosis.json
          path: outputs_kurtosis.json

      - name: Create HDR plots PNG artifacts
        id: create_hdr_plots
        if: always() && steps.parse_kurtosis_output.outputs.success == 'true'
        run: |
          # Check if the JSON file exists
          output_json="${{ github.workspace }}/outputs_kurtosis.json"
          if [ ! -f "$output_json" ]; then
            echo "JSON file not found, skipping"
            exit 0
          fi

          # Check if tx_pool_latency_analysis array exists in the JSON
          if ! jq -e '.tx_pool_latency_analysis' "$output_json" > /dev/null 2>&1; then
            echo "tx_pool_latency_analysis array not found in JSON, skipping"
            exit 0
          fi

          # Get the array length
          array_length=$(jq '.tx_pool_latency_analysis | length' "$output_json")

          # Loop through each element in the array
          for ((i=0; i<array_length; i++)); do
            # Get tx_count for this element
            tx_count=$(jq -r ".tx_pool_latency_analysis[$i].tx_count" "$output_json")
            
            # Check if tx_pool_latency_hdr_plot exists for this element
            if jq -e ".tx_pool_latency_analysis[$i].tx_pool_latency_hdr_plot" "$output_json" > /dev/null 2>&1; then
              # Extract the plot data to a CSV file
              csv_file="tx_pool_latency_analysis_${i}_${tx_count}.csv"
              
              # Extract the plot data, replace literal '\n' with actual newlines and '\t' with actual tabs,
              # and remove all lines starting with '#'
              jq -r ".tx_pool_latency_analysis[$i].tx_pool_latency_hdr_plot" "$output_json" | \
                sed 's/\\n/\n/g; s/\\t/\t/g' | grep -v "^#" > "$csv_file"
              
              # Call the Python script to process the CSV file
              python3 $ERIGON_QA_PATH/test_system/qa-tests/tx-pool/hdr_plot.py \
                --input_file "$csv_file" \
                --output_file "tx_pool_latency_analysis_${i}_${tx_count}.png"
              
              # Delete the CSV file after processing
              rm -f "$csv_file"
              
              echo "Processed tx_pool_latency_analysis[$i] with tx_count=$tx_count"
            else
              echo "tx_pool_latency_hdr_plot not found for element $i, skipping"
            fi
          done
          # Set an output to indicate we processed at least one element
          echo "processed=true" >> $GITHUB_OUTPUT

      - name: Upload PNG images as artifacts
        if: always() && steps.create_hdr_plots.outputs.processed == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: tx-pool-latency-plots
          path: tx_pool_latency_analysis_*.png
          if-no-files-found: ignore

      - name: Clean up PNG files
        if: always() && steps.create_hdr_plots.outputs.processed == 'true'
        run: |
          # Remove all generated PNG files
          rm -f tx_pool_latency_analysis_*.png

      - name: Save test results
        if: always()
        env:
          TEST_RESULT: ${{ steps.erigon_kurtosis_test.outputs.test_result || 'failure' }}
        run: |
          python3 $ERIGON_QA_PATH/test_system/qa-tests/uploads/upload_test_results.py \
            --repo erigon \
            --commit $(git rev-parse HEAD) \
            --branch ${{ github.ref_name }} \
            --test_name txpool-performance \
            --runner ${{ runner.name }} \
            --outcome $TEST_RESULT \
            --result_file ${{ github.workspace }}/outputs_kurtosis.json

      - name: Report results in run summary
        if: always()
        run: |
          output_json="${{ github.workspace }}/outputs_kurtosis.json"

          # Start building the summary
          echo "# TxPool Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check if JSON file exists
          if [ ! -f "$output_json" ]; then
            echo "âŒ **No test results found** - JSON output file is missing" >> $GITHUB_STEP_SUMMARY
            exit 0
          fi

          # Extract test result status
          test_result="${{ steps.erigon_kurtosis_test.outputs.test_result || 'failure' }}"
          if [ "$test_result" = "success" ]; then
            echo "âœ… **Overall Test Status:** PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ **Overall Test Status:** FAILED" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Report throughput analysis
          echo "## ðŸ“Š Throughput Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if jq -e '.tx_pool_throughput_analysis' "$output_json" > /dev/null 2>&1; then
            echo "| TX Count | Mean TPS | Missed Events | Duplicated Events | Omission Events |" >> $GITHUB_STEP_SUMMARY
            echo "|----------|----------|---------------|-------------------|-----------------|" >> $GITHUB_STEP_SUMMARY
            
            throughput_length=$(jq '.tx_pool_throughput_analysis | length' "$output_json")
            for ((i=0; i<throughput_length; i++)); do
              tx_count=$(jq -r ".tx_pool_throughput_analysis[$i].tx_count" "$output_json")
              mean_tps=$(jq -r ".tx_pool_throughput_analysis[$i].mean_tps_throughput" "$output_json")
              missed=$(jq -r ".tx_pool_throughput_analysis[$i].missed_p2p_event_count" "$output_json")
              duplicated=$(jq -r ".tx_pool_throughput_analysis[$i].duplicated_p2p_event_count" "$output_json")
              omission=$(jq -r ".tx_pool_throughput_analysis[$i].coordinated_omission_events_count" "$output_json")
              
              # Format TPS to 2 decimal places
              mean_tps_formatted=$(printf "%.2f" "$mean_tps")
              
              echo "| $tx_count | $mean_tps_formatted | $missed | $duplicated | $omission |" >> $GITHUB_STEP_SUMMARY
            done
          else
            echo "âš ï¸ No throughput analysis data available" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Report latency analysis
          echo "## ðŸ• Latency Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if jq -e '.tx_pool_latency_analysis' "$output_json" > /dev/null 2>&1; then
            echo "| TX Count | Min (Î¼s) | P50 (ms) | P75 (ms) | P99 (ms) | Max (ms) | Missed Events | Duplicated Events |" >> $GITHUB_STEP_SUMMARY
            echo "|----------|----------|----------|----------|----------|----------|---------------|-------------------|" >> $GITHUB_STEP_SUMMARY
            
            latency_length=$(jq '.tx_pool_latency_analysis | length' "$output_json")
            for ((i=0; i<latency_length; i++)); do
              tx_count=$(jq -r ".tx_pool_latency_analysis[$i].tx_count" "$output_json")
              min_us=$(jq -r ".tx_pool_latency_analysis[$i].min_latency_mus" "$output_json")
              max_us=$(jq -r ".tx_pool_latency_analysis[$i].max_latency_mus" "$output_json")
              missed=$(jq -r ".tx_pool_latency_analysis[$i].missed_p2p_event_count" "$output_json")
              duplicated=$(jq -r ".tx_pool_latency_analysis[$i].duplicated_p2p_event_count" "$output_json")
              
              # Convert max from microseconds to milliseconds
              max_ms=$(echo "scale=2; $max_us / 1000" | bc -l)
              
              # Extract percentiles from HDR plot data
              hdr_data=$(jq -r ".tx_pool_latency_analysis[$i].tx_pool_latency_hdr_plot" "$output_json")
              
              # Extract P50, P75, P99 values (convert from microseconds to milliseconds)
              p50_us=$(echo "$hdr_data" | grep "0.500000" | awk '{print $1}' || echo "0")
              p75_us=$(echo "$hdr_data" | grep "0.750000" | awk '{print $1}' || echo "0")
              p99_us=$(echo "$hdr_data" | grep "0.990000\|0.99" | tail -1 | awk '{print $1}' || echo "0")
              
              # Convert to milliseconds and format
              p50_ms=$(echo "scale=2; $p50_us / 1000" | bc -l 2>/dev/null || echo "N/A")
              p75_ms=$(echo "scale=2; $p75_us / 1000" | bc -l 2>/dev/null || echo "N/A")
              p99_ms=$(echo "scale=2; $p99_us / 1000" | bc -l 2>/dev/null || echo "N/A")
              
              echo "| $tx_count | $min_us | $p50_ms | $p75_ms | $p99_ms | $max_ms | $missed | $duplicated |" >> $GITHUB_STEP_SUMMARY
            done
          else
            echo "âš ï¸ No latency analysis data available" >> $GITHUB_STEP_SUMMARY
          fi
          echo "" >> $GITHUB_STEP_SUMMARY

          # Add artifact links
          echo "## ðŸ“ˆ Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- [Raw JSON Results](../actions/runs/${{ github.run_id }}/artifacts)" >> $GITHUB_STEP_SUMMARY
          echo "- [Latency Distribution Plots](../actions/runs/${{ github.run_id }}/artifacts)" >> $GITHUB_STEP_SUMMARY

      - name: Report erigon failure
        if: always()
        run: |
          output_json="${{ github.workspace }}/outputs_kurtosis.json"
          test_result="${{ steps.erigon_kurtosis_test.outputs.test_result || 'failure' }}"

          # Initialize failure reasons array
          failure_reasons=()

          # Check overall test result
          if [ "$test_result" != "success" ]; then
            failure_reasons+=("Overall test execution failed")
          fi

          # Check if JSON output exists
          if [ ! -f "$output_json" ]; then
            failure_reasons+=("No test results generated - possible Erigon crash or startup failure")
          else
            # Analyze JSON for performance issues and anomalies
            
            # Check throughput analysis for concerning metrics
            if jq -e '.tx_pool_throughput_analysis' "$output_json" > /dev/null 2>&1; then
              throughput_length=$(jq '.tx_pool_throughput_analysis | length' "$output_json")
              for ((i=0; i<throughput_length; i++)); do
                tx_count=$(jq -r ".tx_pool_throughput_analysis[$i].tx_count" "$output_json")
                mean_tps=$(jq -r ".tx_pool_throughput_analysis[$i].mean_tps_throughput" "$output_json")
                missed=$(jq -r ".tx_pool_throughput_analysis[$i].missed_p2p_event_count" "$output_json")
                omission=$(jq -r ".tx_pool_throughput_analysis[$i].coordinated_omission_events_count" "$output_json")
                
                # Check for very low throughput (less than 100 TPS)
                if (( $(echo "$mean_tps < 100" | bc -l) )); then
                  failure_reasons+=("Very low throughput detected: ${mean_tps} TPS for ${tx_count} transactions")
                fi
                
                # Check for high missed events (more than 5% of tx_count)
                missed_threshold=$(echo "$tx_count * 0.05" | bc -l)
                if (( $(echo "$missed > $missed_threshold" | bc -l) )); then
                  failure_reasons+=("High missed P2P events: ${missed} (>${missed_threshold} threshold) for ${tx_count} transactions")
                fi
                
                # Check for coordinated omission events
                if [ "$omission" != "0" ] && [ "$omission" != "null" ]; then
                  failure_reasons+=("Coordinated omission events detected: ${omission} for ${tx_count} transactions")
                fi
              done
            fi
            
            # Check latency analysis for concerning metrics
            if jq -e '.tx_pool_latency_analysis' "$output_json" > /dev/null 2>&1; then
              latency_length=$(jq '.tx_pool_latency_analysis | length' "$output_json")
              for ((i=0; i<latency_length; i++)); do
                tx_count=$(jq -r ".tx_pool_latency_analysis[$i].tx_count" "$output_json")
                max_us=$(jq -r ".tx_pool_latency_analysis[$i].max_latency_mus" "$output_json")
                missed=$(jq -r ".tx_pool_latency_analysis[$i].missed_p2p_event_count" "$output_json")
                
                # Check for very high max latency (more than 5 seconds)
                if [ "$max_us" != "null" ] && (( $(echo "$max_us > 5000000" | bc -l) )); then
                  max_sec=$(echo "scale=2; $max_us / 1000000" | bc -l)
                  failure_reasons+=("Very high maximum latency detected: ${max_sec}s for ${tx_count} transactions")
                fi
                
                # Check for high missed events in latency analysis
                missed_threshold=$(echo "$tx_count * 0.05" | bc -l)
                if (( $(echo "$missed > $missed_threshold" | bc -l) )); then
                  failure_reasons+=("High missed P2P events in latency analysis: ${missed} for ${tx_count} transactions")
                fi
              done
            fi
          fi

          # Report failures if any
          if [ ${#failure_reasons[@]} -gt 0 ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## âŒ Erigon Performance Issues Detected" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The following issues were identified:" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            for reason in "${failure_reasons[@]}"; do
              echo "- âš ï¸ $reason" >> $GITHUB_STEP_SUMMARY
            done
            
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Recommendation:** Review Erigon logs, system resources, and network conditions." >> $GITHUB_STEP_SUMMARY
            
            # Set job as failed if critical issues detected
            echo "::error::Erigon performance issues detected: ${failure_reasons[0]}"
            exit 1
          else
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## âœ… No Critical Issues Detected" >> $GITHUB_STEP_SUMMARY
            echo "Erigon performance appears to be within acceptable ranges." >> $GITHUB_STEP_SUMMARY
          fi

      - name: Clean test results
        uses: gacts/run-and-post-run@v1.4.2
        if: always()
        with:
          post: |
            rm -rf kurtosis_artifacts
            rm -f outputs_kurtosis.json
